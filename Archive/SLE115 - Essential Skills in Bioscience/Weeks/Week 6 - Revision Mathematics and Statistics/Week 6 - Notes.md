Chapters 2-9, 16 (numeracy and graphs)
Chapters 25-27 (SI units, moles, pH)
Chapters 30-32, 35, 36-39 (statistics)
Chapters 40-45 (statistical tests - not calculations but knowing when to use them)

In test, you can use a formula sheet

$(2.31 \times 10^{4}) \times (5.4 \times 10^{6})$
-> $2.31 \times 5.4 = 12.47$
-> $10^{4} \times 10^{6} = 10^{10}$
-> = $12.47 \times 10^{10}$
-> = $1.2 \times 10^{11}$  (sig figs)

100,000 eggs/day; 1.2 million worms/km; 3km^2
-> $1.0 \times 10^{5}$; $1.2 \times 10^{6} \rightarrow \times 3 = 3.6 \times 10^{6}$
-> $(3.6 \times 10^{6}) \times (1.0 \times 10^{5}) = 3.6 \times 10^{11} \text{eggs/day/3km}^{2}$

$r>0.9 = \text{strong relationship}$

$y=0.1203x + 0.1542$
Find a given $x$ value by putting it into the equation
$y=1.203(8) + 0.1542 = 1.1166$
	-> $x=8, y=1.1166$

Kelvin to C: K - 273.15
C to Kelvin: C + 273.15

pH: Water is neutral (7)
 -> \[$H^{+}$] = $1 \times 10^{-7} mol$
 -> pH = $-log[H^{+}]$
 -> $[H^{+}]= 10^{-[pH]}$
 -> as pH declines, $[H^{+}]$ increases
 -> Bases decrease hydrogen ions, while acids increase
 \[H^+] = 1M; pH=-log(1)=0
$H^+$ = 0.01, pH = $-log[0.01]=2$
$H^+$ = 0.0022, pH = $-log[0.0022]=2.66=2.7$
$pH=3.0; [H^{+}]=10^{-[3.0]}= 0.001 M= 1.0 \times 10^{-3}$
$pH=7.0; [H^{+}]=10^{-[7.0]}= 0.0000001 M= 1.0 \times 10^{-7}$

Stratified random - classify into strata, sample from each strata
Quantitative (numerical) and qualitative (categorical)
- Continuous -> infinite values within a range
- Discrete -> counting
- Qualitative -> non-numerical (nominal/ordinal only)
	- categorical/attribute (1-5 on a scale, hair colours)
Central tendency: mean/median/mode

Variability - slide 16b
	- Standard deviation
	- Standard Error of the Mean
	- Variance
	- Range
	- Co-efficient of variance
	- Confidence limits (e.g. $\pm 0.01$)

Research/alt hypothesis: Difference between two+ groups
Null hypothesis: No difference between two+ groups
	P-value: Probability of finding observed result if null hypothesis is true
		- The level of confidence in which we can accept the hypothesis
		- Small = greater than chance alone; large = likely chance
			- Target p-value is 0.05 (95% confidence) or lower (lower is more confidence)
				- 0.05 is the standard
		- likelihood of a result occurring by chance
	- $\alpha$ (alpha) If p < $a$, the test is significant, otherwise, it's not

![[alphabet-Greek-sound-equivalents-English.gif]]
Choosing statistical tests: (slides 17-18)
![[Catch Up Maths & Stats, second - Harris, Michael;Taylor, Gordon -exclude.pdf#page=235]]

#### t-test
- Compares samples of normally distributed data with similar $\sigma$
	- usually compares two samples
	- only good for comparing two means (more than two gives a 5% chance of a Type I error (false positive))
		- ANOVA (analysis of variance) is best for this (not on the exam)
Unpaired (independent means): two separate samples, compare means
	- 10 chicken eggs mean to the mean weight of eggs obtained from the farmer next door
Paired (dependent means): Two observations/case (one sample), compare means
	- 10 chicken eggs mean weight on day 1 compared to same eggs the next
One-sample: Single sample, compare mean w/ fixed value
	- 10 chicken eggs and want to know if mean mass is significantly different to the nation standard (which is a fixed value)
Two tailed t-test: Hypothesis - there is a difference (i.e. higher or lower)
	- You want to know *if* there is a difference
One tailed t-test: Hypothesis - one mean is higher than the other (directional)
	- Know the direction of the difference
"Statistically significant difference between the two data sets"


#### Correlation
When there is a linear relationship between two variables, there is a correlation
	- measured by the correlation coefficient '$r$'
	- -1.0 = perfect negative correlation, 1.0=perfect positive, 0 = no relationship
Correlation -> info about strength of association. Not about the cause/effect in the relationship.
	- dependent on the size of the sample (larger sample size=more confidence in even small correlation)
	- only give info about linear relationship. Two variables may have a strong non-linear relationship

Regression analysis - used to quantify the association between two variables (ind-dep)
	- regression line (line of best fit), calculated mathematically
	- linear (straight regression) can be descried by the formula $y=mx+c$


Chi ($\chi$) - square tests
Chi-square \[$\chi$] test is a measure of the difference between actual and expected freqencies
	> Expected frequency: 1:1 male to females
	> Observed: 0.38:0.62 male to females
	
If you have 3 observations  and want to know their values and know nothing about them. Each has the freedom of being unknown, meaning 3 degrees of freedom (df=3)
If we know the value for the mean (e.g. 3), as soon as we know the values of two of the variables, we can calculate the third - meaning 2 degrees of freedom
	> e.g. 10 icy poles w/ 10 flavours. Only need to try 9 before knowing the last one (df=n-1=10-1=9)
	
	